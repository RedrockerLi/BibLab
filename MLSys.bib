% Encoding: UTF-8

@Misc{shoeybi2020megatronlmtrainingmultibillionparameter,
  author        = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  title         = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {1909.08053},
  keywords      = {rank3},
  primaryclass  = {cs.CL},
  review        = {张量并行的拆分方式：减少通信次数和单次通信量。},
  url           = {https://arxiv.org/abs/1909.08053},
}

@Misc{narayanan2021efficientlargescalelanguagemodel,
  author        = {Deepak Narayanan and Mohammad Shoeybi and Jared Casper and Patrick LeGresley and Mostofa Patwary and Vijay Anand Korthikanti and Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and Bryan Catanzaro and Amar Phanishayee and Matei Zaharia},
  title         = {Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2104.04473},
  keywords      = {rank5},
  primaryclass  = {cs.CL},
  review        = {| 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   | 16   | 17   | 18   | 19   | 20   | 21   | 22   | 23   | 24   | 25   | 26   | 27   | 28   | 29   | 30   | 31   | 32   | 33   | 34   | 35   | 36   | 37   | 38   | 39   | 40   | 41   | 42   | 43   | 44   | 45   | 46   | 47   | 48   | 49   | 50   | 51   | 52   | 53   | 54   | 55   | 56   | 57   | 58   | 59   | 60   |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 1a   | 2a   | 3a   | 4a   | 1    | 2    | 3    | 4    | 5a   | 6a   | 7a   | 8a   |      | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 5    | 6    | 7    | 8    | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   |      |      |      |      |      |      |      | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |      |      |      |      | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |
|      | 1a   | 2a   | 3a   | 4a   | 1    | 2    | 3    | 4    | 5a   | 6a   | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 7a   | 8a   |      | 5    | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 6    | 7    | 8    |      |      |      |      | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |      |      |      |      | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |      |      |
|      |      | 1a   | 2a   | 3a   | 4a   | 1    | 2    | 3    | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 4    |      |      |      | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 5a   | 6a   | 7a   | 8a   | 5    | 6    | 7    | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   | 8    |      |      |      | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |      |      |      |      |
|      |      |      | 1a   | 2a   | 3a   | 4a   | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 1    | 2    | 3    | 4    | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | ！   | ！   | ！   | 5a   | 6a   | 7a   | 8a   | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   | 5    | 6    | 7    | 8    | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |      |      |      |      |      |      |

按照**Figure 4**的条件，以“反向传播可以进行就进行，前向传播用于填空”的思路构造，有12段时间被浪费。下面分析性能损失的来源。

一定会被浪费的地方：流水线填充和冲刷。对于最后一个设备，填充和冲刷的时间浪费在一头一尾，中间没有浪费。但上面的构造在28-30的时间浪费在等待5a的前向传播。（每台设备计算时间一样，可以只分析一台）

所以真正的原则是：”不浪费除了流水线填充和冲刷的时间“。这也是可以方便写出浪费时间解析式保证。

下面是构造方法：最后一台设备可以随时发起反向传播，所以要让最后一台尽可能快速的收到更多前向传播的值。但为了能临时保存参数，只能使用之前和microbatchs相同的数据，并把每个设备的资源分为多份，产生更多microbatchs。同时采取了交叉前，反向传播的方式减少了中间值的保存时间。},
  url           = {https://arxiv.org/abs/2104.04473},
}

@Misc{korthikanti2022reducingactivationrecomputationlarge,
  author        = {Vijay Korthikanti and Jared Casper and Sangkug Lym and Lawrence McAfee and Michael Andersch and Mohammad Shoeybi and Bryan Catanzaro},
  title         = {Reducing Activation Recomputation in Large Transformer Models},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2205.05198},
  keywords      = {rank5},
  primaryclass  = {cs.LG},
  review        = {核心思想：找到计算量小而内存消耗大的部分，反向传播时重新计算。

序列并行（引入是为了降低层归一化以及注意力和MLP块后的dropout操作的内存占用$10 ->\frac{10}{t}$）：reduce-scatter+all-gather,通信开销和张量并行规约（a ring all-reduce）相同。},
  url           = {https://arxiv.org/abs/2205.05198},
}

@Comment{jabref-meta: databaseType:bibtex;}
